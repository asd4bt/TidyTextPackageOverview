---
title: "Sentiment Analysis Example"
author: "Alden Summerville"
date: "10/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("/cloud/project/tidytext")
getwd()
```

```{r load-packages, include=FALSE}

library(tidytext)
library(tidyverse)
library(dplyr)
library(readr)
library(ggplot2)
library(plotly)
library(stringr)
library(readxl)
library(wordcloud)
library(reshape2)
library(ggwordcloud)

```

```{r}
#load in debate transcript data
transcript <- read_excel("debate_text3.xlsx")
view(transcript)

```

#The purpose of this example is to analyze the transcript from the recent 2020 Presidential Debate. With tidytext, it is possible to extract the most frequently used words by each candidate, and you can even perform a sentiment analysis on the words used throughout the debate. Using tidytext, I created "word clouds" for each candidate that display their most used words throughout the debate, and I also created a plot that expresses the sentiment of the debate (Trump, Biden, and the moderator included) versus time.

```{r}

#create a "tidy" df from the transcript data by using features like unnest_tokens and stop_words
tidy_transcript <- transcript %>%
  unnest_tokens(word, text) %>% #separates text column from chunks of text to one word rows
  anti_join(stop_words) %>% #excludes ultra common words such as "a", "the", "of", etc.
  group_by(speaker) %>%
  count(word, sort=TRUE) %>% #counts the frequency of each word and sorts from highest to lowest
view(tidy_transcript)


#create a subset of trump's most used words
trump_subset <- tidy_transcript %>% 
  filter(speaker == "President Donald J. Trump")

trump_subset <- trump_subset[-c(2,3,5,6,8,9,10,13,16,18,19,24,29,32,36,38,41,52,53,54,60,61,76,77,78,79,80,89,105,108), ] #removes words that were missed by "stop_words" (these were primarily numbers)
view(trump_subset)
#create a word cloud from the 105 most frequently used words
trump_cloud <- trump_subset %>%
  with(wordcloud(word, n, max.words = 105))

#create a subset of biden's most used words
biden_subset <- tidy_transcript %>% 
  filter(speaker == "Vice President Joe Biden")

biden_subset <- biden_subset[-c(2,3,4,5,7,8,12,13,15,17,19,27,30,34,39,52,65,66,74,84,85,87,88,89,103,104,121,127,128,129,130,131,132), ] #removes words missed by stop_words
view(biden_subset)
#creates word cloud
biden_cloud <- biden_subset %>%
  with(wordcloud(word, n, max.words = 105))

```


```{r}
#plots the sentiment of the debate vs time
unnested <- transcript %>%
  unnest_tokens(word, text) %>% #tidy the data
  anti_join(stop_words) %>%
  mutate(minutes = seconds_in*(1/60)) %>% #create a minutes column
  inner_join(get_sentiments("bing")) #uses the sentiment lexicon "bing" which categorizes words in a binary fashion into positive and negative categories
view(unnested)

debate_sentiment <- unnested %>%
  count(index = minutes %/% 1, sentiment) %>% #creates time index
  spread(sentiment, n, fill = 0) %>% #creates separate columns for positive and negative sentiments
  mutate(sentiment = positive - negative) #calculate new sentiment
view(debate_sentiment)

#plot net sentiment vs time to see the "story" of the debate
ggplot(debate_sentiment, aes(index, sentiment)) +
  geom_col(show.legend = FALSE)+
  xlab("Time Index")+
  ylab("Sentiment")+
  ggtitle("Debate Sentiment vs Time")+
  annotate(geom='text',
           x = 60, y = 7,
           label=glue::glue("Sentiment above x-axis = positive, below = negative"),
           size=4)+ #add label explaining 
  theme_minimal()

#as expected (if you watched the debate) the sentiment is primarily negative throughout the entire debate!

```


